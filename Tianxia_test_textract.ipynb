{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to run this from a terminal:\n",
    "#conda install -c conda-forge poppler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    from PyPDF2 import PdfFileWriter, PdfFileReader\n",
    "except:\n",
    "    !pip install PyPDF2\n",
    "    from PyPDF2 import PdfFileWriter, PdfFileReader\n",
    "    \n",
    "try:\n",
    "    from pdf2image import convert_from_path\n",
    "except:\n",
    "    !pip install pdf2image\n",
    "    from pdf2image import convert_from_path\n",
    "    \n",
    "from pdf2image.exceptions import (\n",
    "    PDFInfoNotInstalledError,\n",
    "    PDFPageCountError,\n",
    "    PDFSyntaxError\n",
    ")\n",
    "\n",
    "try:\n",
    "    from trp import Document\n",
    "except:\n",
    "    !pip install textract-trp\n",
    "    from trp import Document\n",
    "\n",
    "try:\n",
    "    import tabula\n",
    "except:\n",
    "    !pip install tabula-py\n",
    "\n",
    "!pip install pillow\n",
    "from PyPDF2 import PdfFileWriter, PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import boto3\n",
    "from trp import Document\n",
    "import difflib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdffile=r\"f941.pdf\"\n",
    "#r\"UC-CR4 Q2 19 AL SUI-sample3.pdf\"\n",
    "#r\"AL UC-CR4 Q4 2019-sample1.pdf\",\n",
    "#r\"f941.pdf\",\n",
    "#r\"Mississippi 89 140 SIT FLT provided.pdf\",\n",
    "#r\"NMDOLES903ATotalsReport.pdf\",\n",
    "#r\"Oregon OQ Tax Report Q1 2019-sample2.pdf\",\n",
    "#r\"TN LB-0456 Q1 18.pdf\",\n",
    "#r\"UC-CR4 Q2 19 AL SUI-sample3.pdf\",\n",
    "#r\"Handwritten Sample I9.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert PDF into Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdffile=r\"OregonOQ.pdf\"\n",
    "filename=pdffile.split('.')[0]\n",
    "inputpdf = PdfFileReader(open(pdffile, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r /home/ec2-user/SageMaker/pdf_pages/\n",
    "!mkdir -p /home/ec2-user/SageMaker/pdf_pages/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pdf document into single page files, then upload to s3 \n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "BUCKET = \"adptextractpoc\"\n",
    "FOLDER = filename\n",
    "\n",
    "for i in range(inputpdf.numPages):\n",
    "    output = PdfFileWriter()\n",
    "    output.addPage(inputpdf.getPage(i))\n",
    "    with open(\"/home/ec2-user/SageMaker/pdf_pages/pdf-page%s.pdf\" % i, \"wb\") as outputStream:\n",
    "        output.write(outputStream)\n",
    "\n",
    "    \n",
    "for i in range(inputpdf.numPages):    \n",
    "    images = convert_from_path('/home/ec2-user/SageMaker/pdf_pages/pdf-page{}.pdf'.format(i),  500)\n",
    "    for image in images:\n",
    "        image.save('/home/ec2-user/SageMaker/pdf_pages/{}-pdf-page{}.jpg'.format(filename,i), 'JPEG')\n",
    "        s3.upload_file('/home/ec2-user/SageMaker/pdf_pages/{}-pdf-page{}.jpg'.format(filename,i), \n",
    "                       Bucket = BUCKET, \n",
    "                       Key = '{}/{}-pdf-page{}.jpg'.format(filename,filename,i))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of local pdf pages\n",
    "!rm -r /home/ec2-user/SageMaker/pdf_pages/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no need to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttract_client = boto3.client('textract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = \"wsj-tech-internal\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = []\n",
    "conf = []\n",
    "line_class = []\n",
    "\n",
    "for i in range(15):\n",
    "    print(i)\n",
    "    KEY    = 'textract/pdf-page{}.jpg'.format(i)\n",
    "    response = ttract_client.detect_document_text(Document={'S3Object':{'Bucket':BUCKET,'Name':KEY}})\n",
    "    for block in response['Blocks']:\n",
    "        if block[\"BlockType\"] == \"LINE\":\n",
    "            line.append(block.get(\"Text\", \"null\"))\n",
    "            conf.append(block.get(\"Confidence\", \"null\"))\n",
    "            line_class.append(\"bad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY    = 'textract/pdf-page15.jpg'\n",
    "response = ttract_client.detect_document_text(Document={'S3Object':{'Bucket':BUCKET,'Name':KEY}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_1 = 0\n",
    "line_2 = 0\n",
    "for block in response['Blocks']:\n",
    "    if (line_1 == 1) & (line_2 == 1):\n",
    "        if block[\"BlockType\"] == \"LINE\": \n",
    "            line.append(block.get(\"Text\", \"null\"))\n",
    "            conf.append(block.get(\"Confidence\", \"null\"))\n",
    "            line_class.append(\"suspect\")\n",
    "    if (line_1 == 0) & (line_2 == 0):\n",
    "        if block[\"BlockType\"] == \"LINE\":\n",
    "            line.append(block.get(\"Text\", \"null\"))\n",
    "            conf.append(block.get(\"Confidence\", \"null\"))\n",
    "            line_class.append(\"bad\")\n",
    "            if block.get(\"Text\", \"null\") == \"bytch\":\n",
    "                line_1 = 1\n",
    "    if (line_1 == 1) & (line_2 == 0):\n",
    "        if block[\"BlockType\"] == \"LINE\": \n",
    "            if block.get(\"Text\", \"null\") == \"comma-separated list.\":\n",
    "                line_2 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({'Phrase':line, 'Confidence':conf, 'Type':line_class})\n",
    "df['Confidence'] = df['Confidence'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdffile=r\"f941.pdf\"\n",
    "#r\"UC-CR4 Q2 19 AL SUI-sample3.pdf\"\n",
    "#r\"AL UC-CR4 Q4 2019-sample1.pdf\",\n",
    "#r\"f941.pdf\",\n",
    "#r\"Mississippi 89 140 SIT FLT provided.pdf\",\n",
    "#r\"NMDOLES903ATotalsReport.pdf\",\n",
    "#r\"Oregon OQ Tax Report Q1 2019-sample2.pdf\",\n",
    "#r\"TN LB-0456 Q1 18.pdf\",\n",
    "#r\"UC-CR4 Q2 19 AL SUI-sample3.pdf\",\n",
    "#r\"Handwritten Sample I9.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Textract Production Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3BucketName=\"adptextractpoc\"\n",
    "documentName=\"0_AL UC-CR4 Q4 2019-sample1.png\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. raw text extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3BucketName=\"adptextractpoc\"\n",
    "#documentName=\"0_AL UC-CR4 Q4 2019-sample1.png\"\n",
    "\n",
    "pageNo=0\n",
    "pdffile=r\"AL UC-CR4 Q4 2019-sample1.pdf\"\n",
    "filename=pdffile.split('.')[0]\n",
    "documentName='{}/{}-pdf-page{}.jpg'.format(filename,filename,pageNo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# Document\n",
    "#s3BucketName = \"ki-textract-demo-docs\"\n",
    "#documentName = \"simple-document-image.jpg\"\n",
    "\n",
    "# Amazon Textract client\n",
    "textract = boto3.client('textract')\n",
    "\n",
    "# Call Amazon Textract\n",
    "response = textract.detect_document_text(\n",
    "    Document={\n",
    "        'S3Object': {\n",
    "            'Bucket': s3BucketName,\n",
    "            'Name': documentName\n",
    "        }\n",
    "    })\n",
    "\n",
    "#print(response)\n",
    "\n",
    "# Print detected text\n",
    "for item in response[\"Blocks\"]:\n",
    "    if item[\"BlockType\"] == \"WORD\":\n",
    "        print ('\\033[94m' +  item[\"Text\"] + '\\033[0m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### write response to json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## write response to json file:\n",
    "with open('{}-page{}.json'.format(filename,pageNo), 'w') as file:\n",
    "     file.write(json.dumps(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### load existing json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load existing json file:\n",
    "with open('{}-page{}.json'.format(filename,pageNo)) as json_file: \n",
    "    response = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## function find Single Word location (X,Y)\n",
    "WORD2find=\"35242\"\n",
    "for item in response[\"Blocks\"]:\n",
    "    if item[\"BlockType\"] == \"WORD\":\n",
    "        if item[\"Text\"] == WORD2find:\n",
    "            print ('\\033[94m' +  item[\"Text\"] + '\\033[0m')\n",
    "            \n",
    "            print(item)\n",
    "            print(item['Geometry']['Polygon'])\n",
    "            \n",
    "            Word_TopLeftX=item['Geometry']['Polygon'][0]['X']\n",
    "            Word_TopLeftY=item['Geometry']['Polygon'][0]['Y']\n",
    "            Word_BottomRightX=item['Geometry']['Polygon'][2]['X']\n",
    "            Word_BottomRightY=item['Geometry']['Polygon'][2]['Y']\n",
    "            print(\"TopLeftX:\",Word_TopLeftX)\n",
    "            print(\"TopLeftY:\",Word_TopLeftY)\n",
    "            print(\"BottomRightX:\",Word_BottomRightX)\n",
    "            print(\"BottomRightY:\",Word_BottomRightY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function found Line contain certain word(s):\n",
    "WORDS2find=\"AL 35242\"\n",
    "for item in response[\"Blocks\"]:\n",
    "    if item[\"BlockType\"] == \"LINE\":\n",
    "        if WORDS2find in item[\"Text\"]:\n",
    "            print ('\\033[94m' +  item[\"Text\"] + '\\033[0m')\n",
    "            \n",
    "            print(item['Geometry']['Polygon'])\n",
    "            thisLoc=item['Geometry']['Polygon']\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABC_loc=thisLoc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_loc=thisLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABC_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ABC_loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AL_loc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TopLX=ABC_loc[0]['X']\n",
    "TopLY=ABC_loc[0]['Y']\n",
    "BotRX=AL_loc[2]['X']\n",
    "BotRY=AL_loc[2]['Y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function: Give TopLeftX,TopLeftY, BottomRightX,BottomRightY, \n",
    "##           find Lines of words(or Words) in between boundingbox:\n",
    "#TopX=ABC_loc[0]['X']   #TopX of Word:ABC (can use FINDWord or FINDLine to get)\n",
    "#TopY=ABC_loc[0]['Y']\n",
    "#BotX=AL_loc[2]['X']    #BottomX of Word:AL  **3rd element is Bottom Right***\n",
    "#BotY=AL_loc[2]['Y']\n",
    "\n",
    "findList=[]\n",
    "for item in response[\"Blocks\"]:\n",
    "    if item[\"BlockType\"] == \"LINE\": #can use WORD\n",
    "        thisTopLX=item['Geometry']['Polygon'][0]['X']\n",
    "        thisTopLY=item['Geometry']['Polygon'][0]['Y']\n",
    "        thisBotRX=item['Geometry']['Polygon'][2]['X']\n",
    "        thisBotRY=item['Geometry']['Polygon'][2]['Y']\n",
    "        if thisTopLX>=TopLX and thisTopLY>=TopLY and thisBotRX<=BotRX and thisBotRY<=BotRY:\n",
    "            findList.append(item['Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. form extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use trp library to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3BucketName=\"adptextractpoc\"\n",
    "#documentName=\"0_AL UC-CR4 Q4 2019-sample1.png\"\n",
    "#documentName=\"0_AL UC-CR4 Q4 2019-sample1.png\"\n",
    "#documentName=\"941_2020.png\" \n",
    "\n",
    "pageNo=0\n",
    "pdffile=r\"NMDOLES903ATotalsReport.pdf\"\n",
    "filename=pdffile.split('.')[0]\n",
    "documentName='{}/{}-pdf-page{}.jpg'.format(filename,filename,pageNo)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "from trp import Document\n",
    "\n",
    "# Document\n",
    "#s3BucketName = \"ki-textract-demo-docs\"\n",
    "#documentName = \"employmentapp.png\"\n",
    "\n",
    "# Amazon Textract client\n",
    "textract = boto3.client('textract')\n",
    "\n",
    "# Call Amazon Textract\n",
    "response = textract.analyze_document(\n",
    "    Document={\n",
    "        'S3Object': {\n",
    "            'Bucket': s3BucketName,\n",
    "            'Name': documentName\n",
    "        }\n",
    "    },\n",
    "    FeatureTypes=[\"FORMS\"])\n",
    "\n",
    "#print(response)\n",
    "\n",
    "doc = Document(response)\n",
    "\n",
    "form_kv={}\n",
    "for page in doc.pages:\n",
    "    # Print fields\n",
    "    print(\"Fields:\")\n",
    "    for field in page.form.fields:\n",
    "        print(\"Key: {}, Value: {}\".format(field.key, field.value))\n",
    "        try:\n",
    "            form_kv[field.key.text]=field.value.text\n",
    "        except:\n",
    "            form_kv[field.key.text]=None\n",
    "        \n",
    "    # Get field by key\n",
    "#    print(\"\\nGet Field by Key:\")\n",
    "#    key = \"EMPLOYER\"\n",
    "#    field = page.form.getFieldByKey(key)\n",
    "#    if(field):\n",
    "#        print(\"Key: {}, Value: {}\".format(field.key, field.value))\n",
    "\n",
    "    # Search fields by key\n",
    "#    print(\"\\nSearch Fields:\")\n",
    "#    key = \"Wages\"\n",
    "#    fields = page.form.searchFieldsByKey(key)\n",
    "#    for field in fields:\n",
    "#        print(\"Key: {}, Value: {}\".format(field.key, field.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "form_kv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### save extracted K-V to JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}-page{}-formkv.json'.format(filename,pageNo), 'w') as file:\n",
    "     file.write(json.dumps(form_kv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### edit the JSON in VScode editor, correct each Key and Value format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load pre-defined correct form: Key-Value(format) json file\n",
    "with open('NMDOLES903ATotalsReport-page0-formkv-standard.json') as json_file: \n",
    "    form_kv_standard = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "form_kv_standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_kv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### function: correct JSON output Key using predefined Key\n",
    "## input: form_kv - JSON output extracted Key-Value pair dict\n",
    "##        form_kv_standard - this specific form pre-defined Key-value(format) dict\n",
    "## function compare each JSON's Key with predefined Key, and use the close-matched pre-defined key\n",
    "\n",
    "def form_kv_correction(form_kv,form_kv_standard):\n",
    "    form_kv_corrected={}\n",
    "    for key in form_kv.keys():\n",
    "        this_key_correct_matchlist=difflib.get_close_matches(key,form_kv_standard.keys(), n = 1,cutoff = 0.6)\n",
    "        if len(this_key_correct_matchlist)>0:\n",
    "            this_key_correct=this_key_correct_matchlist[0]\n",
    "            form_kv_corrected[this_key_correct]=form_kv[key]\n",
    "        else:\n",
    "            form_kv_corrected[key]=\"NO_MARTCH_KEY_FOUND\"\n",
    "    for key in form_kv_standard.keys():\n",
    "        if key not in form_kv_corrected.keys():\n",
    "            form_kv_corrected[key]=\"NOT_EXTRACTED_KEY\"\n",
    "    return form_kv_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_kv_corrected=form_kv_correction(form_kv,form_kv_standard)\n",
    "form_kv_corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "form_kv_standard.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sort dictionary by key:\n",
    "sorted(form_kv_corrected.items(), key = lambda kv:(kv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check float number \n",
    "if isinstance(2.3, float): print(\"YES\")   # int for integer \n",
    "else: print(\"NO\")\n",
    "    #check string \n",
    "if isinstance('9', str): print(\"YES\")   # int for integer \n",
    "else: print(\"NO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Function: input : ,documentName\n",
    "#          output: extracted form: dataframe of all Key-Value \n",
    "def df_form_keyvalue(s3BucketName,documentName):\n",
    "    textract = boto3.client('textract')\n",
    "    response = textract.analyze_document(\n",
    "    Document={\n",
    "        'S3Object': {\n",
    "            'Bucket': s3BucketName,\n",
    "            'Name': documentName\n",
    "        }\n",
    "    },\n",
    "    FeatureTypes=[\"FORMS\"])\n",
    "    \n",
    "    doc = Document(response)\n",
    "    \n",
    "    df_form=pd.DataFrame(columns=['Key','Value'])\n",
    "    for page in doc.pages:\n",
    "         for field in page.form.fields:\n",
    "            df_form = df_form.append({'Key': field.key, 'Value': field.value}, ignore_index=True)\n",
    "            \n",
    "    return df_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s3BucketName=\"adptextractpoc\"\n",
    "#documentName=\"0_AL UC-CR4 Q4 2019-sample1.png\"\n",
    "documentName=\"941_2020.png\"\n",
    "#documentName=\"941_2020.png\" df_form=df_form_keyvalue(doc)\n",
    "df_form=df_form_keyvalue(s3BucketName,documentName)\n",
    "df_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    df_form=pd.DataFrame(columns=['Key','Value'])\n",
    "    for page in doc.pages:\n",
    "         for field in page.form.fields:\n",
    "            df_form = df_form.append({'Key': field.key, 'Value': field.value}, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a0=df_form.iloc[0]['Value'].text\n",
    "a1=df_form.iloc[1]['Value'].text\n",
    "a2=df_form.iloc[2]['Value'].text\n",
    "a3=df_form.iloc[3]['Value'].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a2l=a2.split(\" \")\n",
    "a2l.insert(2,'.')\n",
    "''.join(a2l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post processing add . inbetween number & remove space in between number \n",
    "# e.g.: $ 108. 79 -> $108.79\n",
    "#       $ 108 79 -> $108.79\n",
    "#       $ 00     -> $00\n",
    "def postporocessing_addDecimalPoint(STR):\n",
    "    nElementSTR=len(STR.split(\" \"))\n",
    "    if STR[0]=='$' and nElementSTR>1:\n",
    "        if nElementSTR==3 and STR.split(\" \")[1][-1]=='.': \n",
    "            STR=STR.replace(\" \",\"\")   #   $ 108. 79 -> $108.79\n",
    "        elif nElementSTR==3 and STR.split(\" \")[1][-1]!='.':\n",
    "            STRlist=STR.split(\" \")\n",
    "            STRlist.insert(2,'.')\n",
    "            STR=''.join(STRlist)   # $ 108 79 -> $108.79\n",
    "        else:\n",
    "            STR=STR.replace(\" \",\"\")  # $ 00 -> $00\n",
    "    return STR\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(postporocessing_addDecimalPoint(a3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function find closed match string to a list of string:\n",
    "# import difflib\n",
    "# print difflib.get_close_matches(\n",
    "#    \"abcd\", [\"abc\", \"acd\", \"abdc\", \"dcba\"])\n",
    "#can work with strings (other than single words). In this case, you need to lower the cutoff (the default is 0.6), \n",
    "#and raise n, the maximum number of matches:\n",
    "\n",
    "import difflib\n",
    "print(difflib.get_close_matches(\n",
    " \"1ST \",                [\"3 Net Taxable Wages (Item 1 minus Item 2)\", \n",
    "                         \"10 Penalty Due\", \n",
    "                         \"dcba\",\n",
    "                         \"Credit Amount\",\n",
    "                         \"1ST MO\",\n",
    "                         \"Employee Tax\",\n",
    "                          \"Net Tax Due\",\n",
    "                         \"2ND MO.\"\n",
    "                       ], n = 4,cutoff = 0.6\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start from JSON output of Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "def get_kv_map(file_name):\n",
    "\n",
    "    with open(file_name, 'rb') as file:\n",
    "        img_test = file.read()\n",
    "        bytes_test = bytearray(img_test)\n",
    "        print('Image loaded', file_name)\n",
    "\n",
    "    # process using image bytes\n",
    "    client = boto3.client('textract')\n",
    "    response = client.analyze_document(Document={'Bytes': bytes_test}, FeatureTypes=['FORMS'])\n",
    "\n",
    "    # Get the text blocks\n",
    "    blocks=response['Blocks']\n",
    "    \n",
    "\n",
    "    # get key and value maps\n",
    "    key_map = {}\n",
    "    value_map = {}\n",
    "    block_map = {}\n",
    "    for block in blocks:\n",
    "        block_id = block['Id']\n",
    "        block_map[block_id] = block\n",
    "        if block['BlockType'] == \"KEY_VALUE_SET\":\n",
    "            if 'KEY' in block['EntityTypes']:\n",
    "                key_map[block_id] = block\n",
    "            else:\n",
    "                value_map[block_id] = block\n",
    "\n",
    "    return key_map, value_map, block_map\n",
    "\n",
    "\n",
    "def get_kv_relationship(key_map, value_map, block_map):\n",
    "    kvs = {}\n",
    "    for block_id, key_block in key_map.items():\n",
    "        value_block = find_value_block(key_block, value_map)\n",
    "        key = get_text(key_block, block_map)\n",
    "        val = get_text(value_block, block_map)\n",
    "        kvs[key] = val\n",
    "    return kvs\n",
    "\n",
    "\n",
    "def find_value_block(key_block, value_map):\n",
    "    for relationship in key_block['Relationships']:\n",
    "        if relationship['Type'] == 'VALUE':\n",
    "            for value_id in relationship['Ids']:\n",
    "                value_block = value_map[value_id]\n",
    "    return value_block\n",
    "\n",
    "\n",
    "def get_text(result, blocks_map):\n",
    "    text = ''\n",
    "    if 'Relationships' in result:\n",
    "        for relationship in result['Relationships']:\n",
    "            if relationship['Type'] == 'CHILD':\n",
    "                for child_id in relationship['Ids']:\n",
    "                    word = blocks_map[child_id]\n",
    "                    if word['BlockType'] == 'WORD':\n",
    "                        text += word['Text'] + ' '\n",
    "                    if word['BlockType'] == 'SELECTION_ELEMENT':\n",
    "                        if word['SelectionStatus'] == 'SELECTED':\n",
    "                            text += 'X '    \n",
    "\n",
    "                                \n",
    "    return text\n",
    "\n",
    "\n",
    "def print_kvs(kvs):\n",
    "    for key, value in kvs.items():\n",
    "        print(key, \":\", value)\n",
    "\n",
    "\n",
    "def search_value(kvs, search_key):\n",
    "    for key, value in kvs.items():\n",
    "        if re.search(search_key, key, re.IGNORECASE):\n",
    "            return value\n",
    "\n",
    "def main(file_name):\n",
    "\n",
    "    key_map, value_map, block_map = get_kv_map(file_name)\n",
    "\n",
    "    # Get Key Value relationship\n",
    "    kvs = get_kv_relationship(key_map, value_map, block_map)\n",
    "    print(\"\\n\\n== FOUND KEY : VALUE pairs ===\\n\")\n",
    "    print_kvs(kvs)\n",
    "\n",
    "    # Start searching a key value\n",
    "    while input('\\n Do you want to search a value for a key? (enter \"n\" for exit) ') != 'n':\n",
    "        search_key = input('\\n Enter a search key:')\n",
    "        print('The value is:', search_value(kvs, search_key))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name='941_2020.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_map, value_map, block_map = get_kv_map(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvs = get_kv_relationship(key_map, value_map, block_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n== FOUND KEY : VALUE pairs ===\\n\")\n",
    "print_kvs(kvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while input('\\n Do you want to search a value for a key? (enter \"n\" for exit) ') != 'n':\n",
    "        search_key = input('\\n Enter a search key:')\n",
    "        print('The value is:', search_value(kvs, search_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def form_kv_from_JSON(response):\n",
    "    blocks=response['Blocks']\n",
    "    key_map = {}\n",
    "    value_map = {}\n",
    "    block_map = {}\n",
    "    for block in blocks:\n",
    "        block_id = block['Id']\n",
    "        block_map[block_id] = block\n",
    "        if block['BlockType'] == \"KEY_VALUE_SET\":\n",
    "            if 'KEY' in block['EntityTypes']:\n",
    "                key_map[block_id] = block\n",
    "            else:\n",
    "                value_map[block_id] = block\n",
    "    kvs = {}\n",
    "    for block_id, key_block in key_map.items():\n",
    "        value_block = find_value_block(key_block, value_map)\n",
    "        key = get_text(key_block, block_map)\n",
    "        val = get_text(value_block, block_map)\n",
    "        kvs[key] = val\n",
    "    return kvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_kv_from_JSON(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Table extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#s3BucketName=\"adptextractpoc\"\n",
    "#documentName=\"0_AL UC-CR4 Q4 2019-sample1.png\"\n",
    "\n",
    "s3BucketName=\"adptextractpoc\"\n",
    "#documentName=\"0_AL UC-CR4 Q4 2019-sample1.png\"\n",
    "#documentName=\"0_AL UC-CR4 Q4 2019-sample1.png\"\n",
    "#documentName=\"941_2020.png\" \n",
    "\n",
    "pageNo=0\n",
    "pdffile=r\"TN LB-0456 Q1 18.pdf\"\n",
    "filename=pdffile.split('.')[0]\n",
    "documentName='{}/{}-pdf-page{}.jpg'.format(filename,filename,pageNo)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use trp library to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from trp import Document\n",
    "\n",
    "# Document\n",
    "#s3BucketName = \"ki-textract-demo-docs\"\n",
    "#documentName = \"expense.png\"\n",
    "\n",
    "# Amazon Textract client\n",
    "textract = boto3.client('textract')\n",
    "\n",
    "# Call Amazon Textract\n",
    "response = textract.analyze_document(\n",
    "    Document={\n",
    "        'S3Object': {\n",
    "            'Bucket': s3BucketName,\n",
    "            'Name': documentName\n",
    "        }\n",
    "    },\n",
    "    FeatureTypes=[\"TABLES\"])\n",
    "\n",
    "#print(response)\n",
    "\n",
    "doc = Document(response)\n",
    "\n",
    "def isFloat(input):\n",
    "  try:\n",
    "    float(input)\n",
    "  except ValueError:\n",
    "    return False\n",
    "  return True\n",
    "\n",
    "warning = \"\"\n",
    "for page in doc.pages:\n",
    "     # Print tables\n",
    "    for table in page.tables:\n",
    "        for r, row in enumerate(table.rows):\n",
    "            itemName  = \"\"\n",
    "            for c, cell in enumerate(row.cells):\n",
    "                print(\"Table[{}][{}] = {}\".format(r, c, cell.text))\n",
    "                if(c == 0):\n",
    "                    itemName = cell.text\n",
    "                elif(c == 4 and isFloat(cell.text)):\n",
    "                    value = float(cell.text)\n",
    "                    if(value > 1000):\n",
    "                        warning += \"{} is greater than $1000.\".format(itemName)\n",
    "if(warning):\n",
    "    print(\"\\nReview needed:\\n====================\\n\" + warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function: input : s3BucketName,documentName\n",
    "#          output: list dataframe of all tables  \n",
    "def df_tables(s3BucketName,documentName):\n",
    "    textract = boto3.client('textract')\n",
    "\n",
    "    response = textract.analyze_document(\n",
    "        Document={\n",
    "            'S3Object': {\n",
    "            'Bucket': s3BucketName,\n",
    "            'Name': documentName\n",
    "            }\n",
    "        },\n",
    "        FeatureTypes=[\"TABLES\"])\n",
    "\n",
    "\n",
    "    doc = Document(response)\n",
    "\n",
    "    df_list= []\n",
    "    for page in doc.pages:\n",
    "        \n",
    "        for table in page.tables:\n",
    "            this_table=[]\n",
    "            for r, row in enumerate(table.rows):\n",
    "                itemName  = \"\"\n",
    "                thisrow=[]\n",
    "                for c, cell in enumerate(row.cells):\n",
    "                    thisrow.append(cell.text)\n",
    "                this_table.append(thisrow)\n",
    "                df = pd.DataFrame(this_table)\n",
    "            df_list.append(df)\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=df_tables(s3BucketName,documentName)\n",
    "print(\"Form Name :\",pdffile)\n",
    "print(\"Page No. :\",pageNo+1)\n",
    "print(\"Number of Tabels:\",len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct OregonOQ.pdf page1 table (4 tables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Table 2: unclear format\n",
    "##### Table 3: can use Form for Key/Value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correct table 4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table=pd.DataFrame()\n",
    "df_table=df[1]\n",
    "df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move 1st row as header\n",
    "df_table.columns = df_table.iloc[0]\n",
    "df_table=df_table[1:]\n",
    "\n",
    "# convert dataframe to a list of dict\n",
    "df_table_dict=df_table.to_dict(orient='records')\n",
    "\n",
    "# write the list of dict to JSON \n",
    "with open('{}-page{}-table4.json'.format(filename,pageNo), 'w') as file:\n",
    "     json.dump(df_table_dict,file)\n",
    "\n",
    "# read JSON (table, list of dict), convert to dataFrame \n",
    "with open('{}-page{}-table4.json'.format(filename,pageNo)) as json_file: \n",
    "    table_json = json.load(json_file)\n",
    "pd.DataFrame(table_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correct Table 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table=pd.DataFrame()\n",
    "df_table=df[2]\n",
    "df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move 1st row as header\n",
    "df_table.columns = df_table.iloc[0]\n",
    "df_table=df_table[1:]\n",
    "\n",
    "# convert dataframe to a list of dict\n",
    "df_table_dict=df_table.to_dict(orient='records')\n",
    "\n",
    "# write the list of dict to JSON \n",
    "with open('{}-page{}-table1.json'.format(filename,pageNo), 'w') as file:\n",
    "     json.dump(df_table_dict,file)\n",
    "\n",
    "# read JSON (table, list of dict), convert to dataFrame \n",
    "with open('{}-page{}-table1.json'.format(filename,pageNo)) as json_file: \n",
    "    table_json = json.load(json_file)\n",
    "pd.DataFrame(table_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct OregonOQ.pdf page2 table (1 table)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table=pd.DataFrame()\n",
    "df_table=df[0]\n",
    "df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#move 1st row as header\n",
    "df_table.columns = df_table.iloc[0]\n",
    "df_table=df_table[1:]\n",
    "\n",
    "#drop column 0 - index \n",
    "df_table=df_table.drop(df_table.columns[0], axis=1)\n",
    "df_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  need to decide last row (21) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct TN LB-0456 Q1 18.pdf page1 table (2 tables)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### correct 1st table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table=df[0]\n",
    "df_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move 1st row as header\n",
    "df_table.columns = df_table.iloc[0]\n",
    "df_table=df_table[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### !!!! date is not correct, need to check either here or later in final output!!!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### correct 2nd table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table=df[1]\n",
    "df_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### !!! need decide the table format (header, columns, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correct UC-CR4 Q2 19 AL SUI-sample3.pdf page3 table (1 table)\n",
    "##### rename column name, write to JSON, and load back as Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Correct UC-CR4 Q2 19 AL SUI-sample3.pdf page 3 table\n",
    "df_table=df[0]\n",
    "\n",
    "#move 1st row as header\n",
    "df_table.columns = df_table.iloc[0]\n",
    "df_table=df_table[1:]\n",
    "\n",
    "# rename column \n",
    "df_table.columns=[\"NAME OF WORKER\", \"WORKERS SOCIAL SECIRITY NUMBER\",\"TOTAL WAGES PAID BEFORE DEDUCTIONS\"]\n",
    "\n",
    "## split Name column into two \n",
    "df_table=df[0]\n",
    "\n",
    "#move 1st row as header\n",
    "df_table.columns = df_table.iloc[0]\n",
    "df_table=df_table[1:]\n",
    "\n",
    "# rename column \n",
    "df_table.columns=[\"NAME OF WORKER\", \"WORKERS SOCIAL SECIRITY NUMBER\",\"TOTAL WAGES PAID BEFORE DEDUCTIONS\"]\n",
    "\n",
    "#no need to run: add a row\n",
    "#df_table=df_table.append([pd.Series([\"ALEX S\", \"111-11-1111\",\"3,212.32\"], index=df_table.columns )],ignore_index=True)\n",
    "\n",
    "\n",
    "## split Name column into two : Last Name and First Initial\n",
    "df_table_name = df_table[\"NAME OF WORKER\"].str.split(expand=True)\n",
    "df_table_name.columns=[\"NAME OF WORKER.LAST NAME\",\"NAME OF WORKER.FIRST INITIAL\"]\n",
    "df_table_new=pd.concat([df_table_name,df_table[[\"WORKERS SOCIAL SECIRITY NUMBER\",\"TOTAL WAGES PAID BEFORE DEDUCTIONS\"]]],axis=1)\n",
    "\n",
    "# convert dataframe to a list of dict\n",
    "df_table_dict=df_table_new.to_dict(orient='records')\n",
    "\n",
    "# write the list of dict to JSON \n",
    "with open('{}-page{}-table.json'.format(filename,pageNo), 'w') as file:\n",
    "     json.dump(df_table_dict,file)\n",
    "\n",
    "# read JSON (table, list of dict), convert to dataFrame \n",
    "with open('{}-page{}-table.json'.format(filename,pageNo)) as json_file: \n",
    "    table_json = json.load(json_file)\n",
    "pd.DataFrame(table_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## if need nested dict, create nested dict : Name of Worker\n",
    "\n",
    "for i in range(len(df_table_dict)):\n",
    "    df_table_dict[i]['NAME of WORKER']={}\n",
    "    df_table_dict[i]['NAME of WORKER']['LAST NAME']=df_table_dict[i]['NAME OF WORKER.LAST NAME']\n",
    "    df_table_dict[i]['NAME of WORKER']['FIRST INITIAL']=df_table_dict[i]['NAME OF WORKER.FIRST INITIAL']\n",
    "    df_table_dict[i].pop('NAME OF WORKER.LAST NAME')\n",
    "    df_table_dict[i].pop('NAME OF WORKER.FIRST INITIAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}-page{}-table.json'.format(filename,pageNo), 'w') as file:\n",
    "     json.dump(df_table_dict,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('UC-CR4 Q2 19 AL SUI-sample3-page2-table.json') as json_file: \n",
    "    table_json = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(table_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start from JSON output of Textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser, os\n",
    "import json\n",
    "import boto3\n",
    "import io\n",
    "from io import BytesIO\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def get_rows_columns_map(table_result, blocks_map):\n",
    "    rows = {}\n",
    "    for relationship in table_result['Relationships']:\n",
    "        if relationship['Type'] == 'CHILD':\n",
    "            for child_id in relationship['Ids']:\n",
    "                cell = blocks_map[child_id]\n",
    "                if cell['BlockType'] == 'CELL':\n",
    "                    row_index = cell['RowIndex']\n",
    "                    col_index = cell['ColumnIndex']\n",
    "                    if row_index not in rows:\n",
    "                        # create new row\n",
    "                        rows[row_index] = {}\n",
    "                        \n",
    "                    # get the text value\n",
    "                    rows[row_index][col_index] = get_text(cell, blocks_map)\n",
    "    return rows\n",
    "\n",
    "\n",
    "def get_text(result, blocks_map):\n",
    "    text = ''\n",
    "    if 'Relationships' in result:\n",
    "        for relationship in result['Relationships']:\n",
    "            if relationship['Type'] == 'CHILD':\n",
    "                for child_id in relationship['Ids']:\n",
    "                    word = blocks_map[child_id]\n",
    "                    if word['BlockType'] == 'WORD':\n",
    "                        text += word['Text'] + ' '\n",
    "                    if word['BlockType'] == 'SELECTION_ELEMENT':\n",
    "                        if word['SelectionStatus'] =='SELECTED':\n",
    "                            text +=  'X '    \n",
    "    return text\n",
    "\n",
    "\n",
    "def get_table_csv_results(file_name):\n",
    "\n",
    "    with open(file_name, 'rb') as file:\n",
    "        img_test = file.read()\n",
    "        bytes_test = bytearray(img_test)\n",
    "        print('Image loaded', file_name)\n",
    "\n",
    "    # process using image bytes\n",
    "    # get the results\n",
    "    client = boto3.client('textract')\n",
    "\n",
    "    response = client.analyze_document(Document={'Bytes': bytes_test}, FeatureTypes=['TABLES'])\n",
    "\n",
    "    # Get the text blocks\n",
    "    blocks=response['Blocks']\n",
    "#    pprint(blocks)\n",
    "\n",
    "    blocks_map = {}\n",
    "    table_blocks = []\n",
    "    for block in blocks:\n",
    "        blocks_map[block['Id']] = block\n",
    "        if block['BlockType'] == \"TABLE\":\n",
    "            table_blocks.append(block)\n",
    "\n",
    "    if len(table_blocks) <= 0:\n",
    "        return \"<b> NO Table FOUND </b>\"\n",
    "\n",
    "    csv = ''\n",
    "    for index, table in enumerate(table_blocks):\n",
    "        csv += generate_table_csv(table, blocks_map, index +1)\n",
    "        csv += '\\n\\n'\n",
    "\n",
    "    return csv\n",
    "\n",
    "def generate_table_csv(table_result, blocks_map, table_index):\n",
    "    rows = get_rows_columns_map(table_result, blocks_map)\n",
    "\n",
    "    table_id = 'Table_' + str(table_index)\n",
    "    \n",
    "    # get cells.\n",
    "    csv = 'Table: {0}\\n\\n'.format(table_id)\n",
    "\n",
    "    for row_index, cols in rows.items():\n",
    "        \n",
    "        for col_index, text in cols.items():\n",
    "            csv += '{}'.format(text) + \",\"\n",
    "        csv += '\\n'\n",
    "        \n",
    "    csv += '\\n\\n\\n'\n",
    "    return csv\n",
    "\n",
    "def main(file_name):\n",
    "    table_csv = get_table_csv_results(file_name)\n",
    "\n",
    "    output_file = 'output.csv'\n",
    "\n",
    "    # replace content\n",
    "    with open(output_file, \"wt\") as fout:\n",
    "        fout.write(table_csv)\n",
    "\n",
    "    # show the results\n",
    "    print('CSV OUTPUT FILE: ', output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "table_csv = get_table_csv_results(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textract Response Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  You can use Textract response parser library to easily parser JSON returned by Amazon Textract. Library parses JSON and provides programming language specific constructs to work with different parts of the document. textractor is an example of PoC batch processing tool that takes advantage of Textract response parser library and generate output in multiple formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3BucketName=\"adptextractpoc\"\n",
    "#documentName=\"0_AL UC-CR4 Q4 2019-sample1.png\"\n",
    "documentName=\"941_2020.png\"\n",
    "#documentName=\"0_f941_filll.png\" \n",
    "\n",
    "textract = boto3.client('textract')\n",
    "\n",
    "# Call Amazon Textract\n",
    "response = textract.analyze_document(\n",
    "    Document={\n",
    "        'S3Object': {\n",
    "            'Bucket': s3BucketName,\n",
    "            'Name': documentName\n",
    "        }\n",
    "    },\n",
    "    FeatureTypes=[\"TABLES\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Amazon Textract and get JSON response\n",
    "#  client = boto3.client('textract')\n",
    "#  response = client.analyze_document(Document={...}, FeatureTypes=[...])\n",
    "\n",
    "# Parse JSON response from Textract\n",
    "doc = Document(response)\n",
    "\n",
    "# Iterate over elements in the document\n",
    "for page in doc.pages:\n",
    "    # Print lines and words\n",
    "#    for line in page.lines:\n",
    "#        print(\"Line: {}--{}\".format(line.text, line.confidence))\n",
    "#        for word in line.words:\n",
    "#            print(\"Word: {}--{}\".format(word.text, word.confidence))\n",
    "\n",
    "    # Print tables\n",
    "    for table in page.tables:\n",
    "        for r, row in enumerate(table.rows):\n",
    "            for c, cell in enumerate(row.cells):\n",
    "                print(\"Table[{}][{}] = {}-{}\".format(r, c, cell.text, cell.confidence))\n",
    "\n",
    "    # Print fields\n",
    "    for field in page.form.fields:\n",
    "        print(\"Field: Key: {}, Value: {}\".format(field.key.text, field.value.text))\n",
    "\n",
    "    # Get field by key\n",
    "    key = \"Phone Number:\"\n",
    "    field = page.form.getFieldByKey(key)\n",
    "    if(field):\n",
    "        print(\"Field: Key: {}, Value: {}\".format(field.key, field.value))\n",
    "\n",
    "    # Search fields by key\n",
    "    key = \"address\"\n",
    "    fields = page.form.searchFieldsByKey(key)\n",
    "    for field in fields:\n",
    "        print(\"Field: Key: {}, Value: {}\".format(field.key, field.value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latest Development Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table extraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import uuid\n",
    "import hashlib\n",
    "import requests\n",
    "import json\n",
    "import warnings\n",
    " \n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    " \n",
    " \n",
    "retry_strategy = Retry(total=3,\n",
    "                       backoff_factor=0.1,\n",
    "                       status_forcelist=[500, 502, 503, 504])\n",
    " \n",
    "warnings.filterwarnings('ignore')\n",
    "class ALICERequestExecutor():\n",
    " \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        :param logger: logger object required by ALICEClient.\n",
    "        :param endpoint: string as ALICE endpoint to hit.\n",
    "        :param region: string as region for S3 resource.\n",
    "        \"\"\"\n",
    "        self._endpoint = 'https://tabledetectortd2-alpha-1500297725.us-east-1.elb.amazonaws.com/theta/invocations'\n",
    " \n",
    "    def _create_payload_data(self, input_bytes):\n",
    "        request_id = str(uuid.uuid4())\n",
    "        return {\n",
    "            'RequestId': request_id,\n",
    "            'InferenceRequests': [\n",
    "                {\n",
    "                    'ImageHash': hashlib.sha256(input_bytes).hexdigest(),\n",
    "                    'RequestId': request_id,\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    " \n",
    "    def execute(self, input_bytes: bytes) -> bytes:\n",
    "        \"\"\"\n",
    "        Take in image bytes, and Call ALICE client with a new request context.\n",
    "        :param input_bytes: bytes as image bytes passed to ALICE endpoint.\n",
    "        :return: bytes as converted response returned from ALICE client.\n",
    "        \"\"\"\n",
    "        session = requests.session()\n",
    "        session.mount(self.endpoint, HTTPAdapter(max_retries=retry_strategy))\n",
    "        files = {'uploadfile': input_bytes}\n",
    "        payload = self._create_payload_data(input_bytes)\n",
    "        data = {\"inference_batch_request\": json.dumps(payload)}\n",
    "        alice_response = session.post(self.endpoint, files=files, data=data, verify=False)\n",
    "        return alice_response.content\n",
    " \n",
    "    @property\n",
    "    def endpoint(self):\n",
    "        return self._endpoint\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    alice_executor = ALICERequestExecutor()\n",
    "    image_path = '941_2020.png'\n",
    "    with open(image_path, 'rb') as file:\n",
    "        bytes = file.read()\n",
    "    output_bytes  = alice_executor.execute(bytes)\n",
    "    alice_output = json.loads(output_bytes, encoding='utf-8')\n",
    "    results = alice_output['InferenceResponses'][0]['Results']\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "blocks=results['Blocks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = Document(results)\n",
    "for page in doc.pages:\n",
    "     # Print tables\n",
    "    for table in page.tables:\n",
    "        for r, row in enumerate(table.rows):\n",
    "            itemName  = \"\"\n",
    "            for c, cell in enumerate(row.cells):\n",
    "                print(\"Table[{}][{}] = {}\".format(r, c, cell.text))\n",
    "                if(c == 0):\n",
    "                    itemName = cell.text\n",
    "                elif(c == 4 and isFloat(cell.text)):\n",
    "                    value = float(cell.text)\n",
    "                    if(value > 1000):\n",
    "                        warning += \"{} is greater than $1000.\".format(itemName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code: Analyze text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/textract/latest/dg/analyzing-document-text.html\n",
    "#Analyzes text in a document stored in an S3 bucket. Display polygon box around text and angled text \n",
    "import boto3\n",
    "import io\n",
    "from io import BytesIO\n",
    "import sys\n",
    "\n",
    "import math\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def ShowBoundingBox(draw,box,width,height,boxColor):\n",
    "             \n",
    "    left = width * box['Left']\n",
    "    top = height * box['Top'] \n",
    "    draw.rectangle([left,top, left + (width * box['Width']), top +(height * box['Height'])],outline=boxColor)   \n",
    "\n",
    "def ShowSelectedElement(draw,box,width,height,boxColor):\n",
    "             \n",
    "    left = width * box['Left']\n",
    "    top = height * box['Top'] \n",
    "    draw.rectangle([left,top, left + (width * box['Width']), top +(height * box['Height'])],fill=boxColor)  \n",
    "\n",
    "# Displays information about a block returned by text detection and text analysis\n",
    "def DisplayBlockInformation(block):\n",
    "    print('Id: {}'.format(block['Id']))\n",
    "    if 'Text' in block:\n",
    "        print('    Detected: ' + block['Text'])\n",
    "    print('    Type: ' + block['BlockType'])\n",
    "   \n",
    "    if 'Confidence' in block:\n",
    "        print('    Confidence: ' + \"{:.2f}\".format(block['Confidence']) + \"%\")\n",
    "\n",
    "    if block['BlockType'] == 'CELL':\n",
    "        print(\"    Cell information\")\n",
    "        print(\"        Column:\" + str(block['ColumnIndex']))\n",
    "        print(\"        Row:\" + str(block['RowIndex']))\n",
    "        print(\"        Column Span:\" + str(block['ColumnSpan']))\n",
    "        print(\"        RowSpan:\" + str(block['ColumnSpan']))    \n",
    "    \n",
    "    if 'Relationships' in block:\n",
    "        print('    Relationships: {}'.format(block['Relationships']))\n",
    "    print('    Geometry: ')\n",
    "    print('        Bounding Box: {}'.format(block['Geometry']['BoundingBox']))\n",
    "    print('        Polygon: {}'.format(block['Geometry']['Polygon']))\n",
    "    \n",
    "    if block['BlockType'] == \"KEY_VALUE_SET\":\n",
    "        print ('    Entity Type: ' + block['EntityTypes'][0])\n",
    "    \n",
    "    if block['BlockType'] == 'SELECTION_ELEMENT':\n",
    "        print('    Selection element detected: ', end='')\n",
    "\n",
    "        if block['SelectionStatus'] =='SELECTED':\n",
    "            print('Selected')\n",
    "        else:\n",
    "            print('Not selected')    \n",
    "    \n",
    "    if 'Page' in block:\n",
    "        print('Page: ' + block['Page'])\n",
    "    print()\n",
    "\n",
    "def process_text_analysis(bucket, document):\n",
    "\n",
    "    #Get the document from S3\n",
    "    s3_connection = boto3.resource('s3')\n",
    "                          \n",
    "    s3_object = s3_connection.Object(bucket,document)\n",
    "    s3_response = s3_object.get()\n",
    "\n",
    "    stream = io.BytesIO(s3_response['Body'].read())\n",
    "    image=Image.open(stream)\n",
    "\n",
    "    # Analyze the document\n",
    "    client = boto3.client('textract')\n",
    "    \n",
    "    image_binary = stream.getvalue()\n",
    "    response = client.analyze_document(Document={'Bytes': image_binary},\n",
    "        FeatureTypes=[\"TABLES\", \"FORMS\"])\n",
    "  \n",
    "\n",
    "    # Alternatively, process using S3 object\n",
    "    #response = client.analyze_document(\n",
    "    #    Document={'S3Object': {'Bucket': bucket, 'Name': document}},\n",
    "    #    FeatureTypes=[\"TABLES\", \"FORMS\"])\n",
    "\n",
    "    \n",
    "    #Get the text blocks\n",
    "    blocks=response['Blocks']\n",
    "    width, height =image.size  \n",
    "    draw = ImageDraw.Draw(image)  \n",
    "    print ('Detected Document Text')\n",
    "   \n",
    "    # Create image showing bounding box/polygon the detected lines/text\n",
    "    for block in blocks:\n",
    "\n",
    "        DisplayBlockInformation(block)\n",
    "             \n",
    "        draw=ImageDraw.Draw(image)\n",
    "        if block['BlockType'] == \"KEY_VALUE_SET\":\n",
    "            if block['EntityTypes'][0] == \"KEY\":\n",
    "                ShowBoundingBox(draw, block['Geometry']['BoundingBox'],width,height,'red')\n",
    "            else:\n",
    "                ShowBoundingBox(draw, block['Geometry']['BoundingBox'],width,height,'green')  \n",
    "            \n",
    "        if block['BlockType'] == 'TABLE':\n",
    "            ShowBoundingBox(draw, block['Geometry']['BoundingBox'],width,height, 'blue')\n",
    "\n",
    "        if block['BlockType'] == 'CELL':\n",
    "            ShowBoundingBox(draw, block['Geometry']['BoundingBox'],width,height, 'yellow')\n",
    "        if block['BlockType'] == 'SELECTION_ELEMENT':\n",
    "            if block['SelectionStatus'] =='SELECTED':\n",
    "                ShowSelectedElement(draw, block['Geometry']['BoundingBox'],width,height, 'blue')    \n",
    "   \n",
    "            #uncomment to draw polygon for all Blocks\n",
    "            #points=[]\n",
    "            #for polygon in block['Geometry']['Polygon']:\n",
    "            #    points.append((width * polygon['X'], height * polygon['Y']))\n",
    "            #draw.polygon((points), outline='blue')\n",
    "            \n",
    "    # Display the image\n",
    "    image.show()\n",
    "    return len(blocks)\n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    bucket = 'adptextractpoc'\n",
    "    document = '941_2020.png'\n",
    "\n",
    "\n",
    "    block_count=process_text_analysis(bucket,document)\n",
    "    print(\"Blocks detected: \" + str(block_count))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example code: Detect text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detects text in a document stored in an S3 bucket. Display polygon box around text and angled text \n",
    "import boto3\n",
    "import io\n",
    "from io import BytesIO\n",
    "import sys\n",
    "\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "import math\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "# Displays information about a block returned by text detection and text analysis\n",
    "def DisplayBlockInformation(block):\n",
    "    print('Id: {}'.format(block['Id']))\n",
    "    if 'Text' in block:\n",
    "        print('    Detected: ' + block['Text'])\n",
    "    print('    Type: ' + block['BlockType'])\n",
    "   \n",
    "    if 'Confidence' in block:\n",
    "        print('    Confidence: ' + \"{:.2f}\".format(block['Confidence']) + \"%\")\n",
    "\n",
    "    if block['BlockType'] == 'CELL':\n",
    "        print(\"    Cell information\")\n",
    "        print(\"        Column:\" + str(block['ColumnIndex']))\n",
    "        print(\"        Row:\" + str(block['RowIndex']))\n",
    "        print(\"        Column Span:\" + str(block['ColumnSpan']))\n",
    "        print(\"        RowSpan:\" + str(block['ColumnSpan']))    \n",
    "    \n",
    "    if 'Relationships' in block:\n",
    "        print('    Relationships: {}'.format(block['Relationships']))\n",
    "    print('    Geometry: ')\n",
    "    print('        Bounding Box: {}'.format(block['Geometry']['BoundingBox']))\n",
    "    print('        Polygon: {}'.format(block['Geometry']['Polygon']))\n",
    "    \n",
    "    if block['BlockType'] == \"KEY_VALUE_SET\":\n",
    "        print ('    Entity Type: ' + block['EntityTypes'][0])\n",
    "    if 'Page' in block:\n",
    "        print('Page: ' + block['Page'])\n",
    "    print()\n",
    "\n",
    "def process_text_detection(bucket, document):\n",
    "\n",
    "    \n",
    "    #Get the document from S3\n",
    "    s3_connection = boto3.resource('s3')\n",
    "                          \n",
    "    s3_object = s3_connection.Object(bucket,document)\n",
    "    s3_response = s3_object.get()\n",
    "\n",
    "    stream = io.BytesIO(s3_response['Body'].read())\n",
    "    image=Image.open(stream)\n",
    "\n",
    "   \n",
    "    # Detect text in the document\n",
    "    \n",
    "    client = boto3.client('textract')\n",
    "    #process using image bytes                      \n",
    "    #image_binary = stream.getvalue()\n",
    "    #response = client.detect_document_text(Document={'Bytes': image_binary})\n",
    "\n",
    "    #process using S3 object\n",
    "    response = client.detect_document_text(\n",
    "        Document={'S3Object': {'Bucket': bucket, 'Name': document}})\n",
    "\n",
    "    #Get the text blocks\n",
    "    blocks=response['Blocks']\n",
    "    width, height =image.size  \n",
    "    draw = ImageDraw.Draw(image)  \n",
    "    print ('Detected Document Text')\n",
    "   \n",
    "    # Create image showing bounding box/polygon the detected lines/text\n",
    "    for block in blocks:\n",
    "            print('Type: ' + block['BlockType'])\n",
    "            if block['BlockType'] != 'PAGE':\n",
    "                print('Detected: ' + block['Text'])\n",
    "                print('Confidence: ' + \"{:.2f}\".format(block['Confidence']) + \"%\")\n",
    "\n",
    "            print('Id: {}'.format(block['Id']))\n",
    "            if 'Relationships' in block:\n",
    "                print('Relationships: {}'.format(block['Relationships']))\n",
    "            print('Bounding Box: {}'.format(block['Geometry']['BoundingBox']))\n",
    "            print('Polygon: {}'.format(block['Geometry']['Polygon']))\n",
    "            print()\n",
    "            draw=ImageDraw.Draw(image)\n",
    "            # Draw WORD - Green -  start of word, red - end of word\n",
    "            if block['BlockType'] == \"WORD\":\n",
    "                draw.line([(width * block['Geometry']['Polygon'][0]['X'],\n",
    "                height * block['Geometry']['Polygon'][0]['Y']),\n",
    "                (width * block['Geometry']['Polygon'][3]['X'],\n",
    "                height * block['Geometry']['Polygon'][3]['Y'])],fill='green',\n",
    "                width=2)\n",
    "            \n",
    "                draw.line([(width * block['Geometry']['Polygon'][1]['X'],\n",
    "                height * block['Geometry']['Polygon'][1]['Y']),\n",
    "                (width * block['Geometry']['Polygon'][2]['X'],\n",
    "                height * block['Geometry']['Polygon'][2]['Y'])],\n",
    "                fill='red',\n",
    "                width=2)    \n",
    "\n",
    "                 \n",
    "            # Draw box around entire LINE  \n",
    "            if block['BlockType'] == \"LINE\":\n",
    "                points=[]\n",
    "\n",
    "                for polygon in block['Geometry']['Polygon']:\n",
    "                    points.append((width * polygon['X'], height * polygon['Y']))\n",
    "\n",
    "                draw.polygon((points), outline='black')    \n",
    "  \n",
    "                # Uncomment to draw bounding box\n",
    "                #box=block['Geometry']['BoundingBox']                    \n",
    "                #left = width * box['Left']\n",
    "                #top = height * box['Top']           \n",
    "                #draw.rectangle([left,top, left + (width * box['Width']), top +(height * box['Height'])],outline='black') \n",
    "\n",
    "\n",
    "    # Display the image\n",
    "    image.show()\n",
    "    # display image for 10 seconds\n",
    "\n",
    "    \n",
    "    return len(blocks)\n",
    "\n",
    "def main():\n",
    "\n",
    "    bucket='adptextractpoc'\n",
    "    document = '941_2020.png'\n",
    "    block_count=process_text_detection(bucket,document)\n",
    "    print(\"Blocks detected: \" + str(block_count))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of Tabula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula\n",
    "\n",
    "# Read pdf into list of DataFrame\n",
    "df = tabula.read_pdf(pdffile, pages='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
